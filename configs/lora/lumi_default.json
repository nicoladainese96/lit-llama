{
  "data_dir": "data/alpaca",
  "pretrained_path": "/pfs/lustref1/flash/project_462000219/llm_models/lit_llama/checkpoints/lit-llama/7B/lit-llama.pth",
  "tokenizer_path": "/pfs/lustref1/flash/project_462000219/ll_models/lit_llama/checkpoints/lit-llama/tokenizer.model",
  "finetuned_name":"lit-llama-lora-finetuned.pth",
  "out_dir": "out/lora/alpaca",
  "instruction_tuning": true,
  "eval_interval": 100,
  "save_interval": 100,
  "eval_iters": 100,
  "log_interval": 1,
  "learning_rate": 0.0003,
  "batch_size": 128,
  "micro_batch_size": 4,
  "gradient_accumulation_iters": 32,
  "max_iters": 18750,
  "weight_decay": 0.0,
  "max_seq_length": 256,
  "lora_r": 8,
  "lora_alpha": 16,
  "lora_dropout": 0.05,
  "warmup_iters": 100
}

